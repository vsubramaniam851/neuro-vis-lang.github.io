<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Revealing Vision-Language Integration in the Brain with Multimodal Networks">
  <meta name="keywords" content="Multimodal, Neuroscience">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Revealing Vision-Language Integration in the Brain with Multimodal Networks</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Revealing Vision-Language Integration in the Brain with Multimodal Networks</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Vighnesh Subramaniam</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://colinconwell.github.io/">Colin Conwell</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://czlwang.github.io/">Christopher Wang</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://klab.tch.harvard.edu/">Gabriel Kreiman</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.csail.mit.edu/boris/boris.html">Boris Katz</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://stanford.edu/~cases/">Ignacio Cases</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="http://0xab.com/">Andrei Barbu</a><sup>1,2</sup>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>MIT CSAIL,</span>
            <span class="author-block"><sup>2</sup>CBMM,</span>
            <span class="author-block"><sup>3</sup>Department of Cognitive Science, Johns Hopkins University,</span>
            <span class="author-block"><sup>4</sup>Boston Childrenâ€™s Hospital, Harvard Medical School</span>
          </div> 

          <div class="is-size-3 publication-authors">
            <span class="author-block">ICML 2024</span>
          </div> 

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2406.14481.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2406.14481"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
              	<a href="https://github.com/vsubramaniam851/brain-multimodal" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
              	</a>  
              </span>
            
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We use (multi)modal deep neural networks (DNNs) to probe for sites of multimodal integration in the human brain by predicting stereoencephalography (SEEG) recordings taken while human subjects watched movies.  We operationalize sites of multimodal integration as regions where a multimodal vision-language model predicts recordings better than unimodal language, unimodal vision, or linearly-integrated language-vision models. Our target DNN models span different architectures (e.g., convolutional networks and transformers) and multimodal training techniques (e.g., cross-attention and contrastive learning). As a key enabling step, we first demonstrate that trained vision and language models systematically outperform their randomly initialized counterparts in their ability to predict SEEG signals. We then compare unimodal and multimodal models against one another. Because our target DNN models often have different architectures, number of parameters, and training sets (possibly obscuring those differences attributable to integration), we carry out a controlled comparison of two models (SLIP and SimCLR), which keep all of these attributes the same aside from input modality. Using this approach, we identify a sizable number of neural sites (on average 141 out of 1090 total sites or 12.94%) and brain regions where multimodal integration seems to occur. Additionally, we find that among the variants of multimodal training techniques we assess, CLIP-style training is the best suited for downstream prediction of the neural activity in these sites.</p>
          <hr>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <figure>
            <img src="./static/figures/new_overview.png" />
            <figcaption style="font-size: 0.8em;"></figcaption>
        </figure>
    	<div class="content has-text-justified">
        <p><b>Multimodal Integration in the Brain Overview.</b> (A) We parse the stimuli, movies, into image-text pairs (which we call \emph{event structures}) and process these with either a vision model, text model, or multimodal model. We extract feature vectors from these models and predict neural activity in 161 25ms time bins per electrode, obtaining a Pearson correlation coefficient per time bin per electrode per model. We exclude any time bins in which a bootstrapping test (computed over event structures) suggests an absence of meaningful signal in the neural activity target in that bin. We run these regressions using both trained and randomly initialized encoders and for two datasets, a vision-aligned dataset and language-aligned dataset, which differ in the methods to sample these pairs. (B) The first analysis of this data investigates if trained models outperform randomly initialized models. The second analysis investigates if multimodal models outperform unimodal models. The third analysis repeats the second holding constant the architecture and dataset to factor out these confounds. A final analysis investigates if multimodal models that meaningfully integrate vision and language features outperform models that simply concatenate them.</p>
        <hr>
    </div>
    </div>
</div>
</section>

<section class="section" style="margin-top: -5px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
	<h3 class="title is-4">Data Overview</h3>
    </div>
   </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <figure>
            <img src="./static/figures/data_overview.png" />
            <figcaption style="font-size: 0.8em;"></figcaption>
        </figure>
        <div class="content has-text-justified">
          <p><b>Data Overview</b> Our dataset consists of stereoelectroencephalography recordings while subjects watch movies. (a) The electrode placements over all subjects. Each yellow dot denotes an electrode collecting invasive field
            potential recordings for further analysis in our experiments. (b) An overview of our data collection procedure. Subjects are presented
            feature length films while neural data is collected from these electrodes in the brain.</p>
          <hr>
      </div>
    </div>
  </div>
</section>

<section class="section" style="margin-top: -5px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
	<h3 class="title is-4">Multimodal Integration Sites</h3>
    </div>
   </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <figure>
            <img src="./static/figures/mult_uni_brains.png" />
            <figcaption style="font-size: 0.8em;"></figcaption>
        </figure>
        <div class="content has-text-justified">
          <p><b>Multimodal Integration Sites.</b> We visualize our identified sites of multimodal integration accordings to 5 tests of multimodality. Multimodal sites aggregated into regions from the DKT atlas. For each site we compute the percentage of multimodal electrodes using the first test and the (left) language or (right) vision alignment. The top defines a site to be multimodal if the best model that explains that electrode is multimodal as opposed to unimodal. The bottom controls for architecture, parameters, and datasets by comparing SLIP-Combo and SLIP-SimCLR. Red regions have no multimodal electrodes. Regions which have at least one electrode that is multimodal both with the vision and language aligned stimuli are marked with a blue star. We notice that many electrodes occur in the temporoparietal junction with a cluster in the superior temporal cortex, middle temporal cortex, inferior parietal lobe, etc. Other areas we identify include the insula, supramarginal cortex, the superior frontal cortex, and the caudal middle frontal cortex.</p>
          <hr>
      </div>
    </div>
  </div>
</section>

<section class="section" style="margin-top: -5px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
	<h3 class="title is-4">Best Multimodal Integration Model</h3>
    </div>
   </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <figure>
            <img src="./static/figures/best_multi_model.png" />
            <figcaption style="font-size: 0.8em;"></figcaption>
        </figure>
        <div class="content has-text-justified">
          <p><b>Best model of multimodal integration.</b> We visualize the individual electrodes that pass our multimodality tests for the language-aligned (top) and vision-aligned datasets (bottom), adding a bold outline to electrodes that pass across both datasets. We color the electrodes by the top-ranked multimodal model that predicts activity in the electrode. We see that models such as SLIP-Combo and SLIP-CLIP often predict activity the best across datasets. We also see that BLIP and Flava are the best architecturally multimodal models.</p>
          <hr>
      </div>
    </div>
  </div>
</section>

<section class="section" style="margin-top: -5px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
	<h3 class="title is-4">Trained vs Randomly Initialized Models</h3>
    </div>
   </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <figure>
            <img src="./static/figures/trained_random_violin.png" />
            <figcaption style="font-size: 0.8em;"></figcaption>
        </figure>
        <div class="content has-text-justified">
          <p><b>Trained models beat randomly initialized models.</b> A comparison between pretrained and randomly initialized model performance showing the distribution of predictivity across electrodes. This averages significant time bins per electrode (where the lower validation confidence interval must be greater than zero), for both datasets alignments and for each of our 12 models. Every trained network outperforms its randomly initialized counterpart. Trained networks overall outperform untrained networks. This is true both on average, and for almost every single electrode.</p>
          <hr>
      </div>
    </div>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{subramaniam2024revealing,
      title={Revealing Vision-Language Integration in the Brain Using Multimodal Networks},
      author={Subramaniam, Vighnesh and Conwell, Colin and Wang, Christopher and Kreiman, Gabriel and Katz, Boris and Cases, Ignacio and Barbu, Andrei},
      booktitle={Fourty-first International Conference on Machine Learning},
      year={2024}
    }
    </code></pre>
  </div>
</section>

	
<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content" style="text-align: center;">
          <p>
            This website is adapted from the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies template</a>, which you are free to borrow if you link back to it in the footer.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>
	


</body>
</html>
